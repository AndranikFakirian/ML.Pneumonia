{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f515f6-0554-4178-bd99-547668d5de18",
   "metadata": {},
   "source": [
    "# Machine Learning model for the pneumonia detection\n",
    "\n",
    "**Author:** [Andranik Fakirian](https://www.tumblr.com/andranikfakirian)<br>\n",
    "**Date created:** 2024/12/06<br>\n",
    "**Last modified:** 2025/1/17<br>\n",
    "**Description:** Create a 3D convolutional neural network using self-supervised learning methods to predict presence of pneumonia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50bdc2b-392a-45b2-bed5-bf964082f53e",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This clean project code shows the steps used to build a 3D convolutional neural network (CNN)\n",
    "to predict the presence of viral pneumonia in computer tomography (CT) scans using self-supervised learning methods. To do this, firstly, build an autoencoder model using multiple lung datasets and augmentation methods. And secondly, build a classifier using the previously built autoencoder as a preset and the pneumonia dataset.\n",
    "\n",
    "## References\n",
    "\n",
    "References from the basic code\n",
    "\n",
    "- [A survey on Deep Learning Advances on Different 3D DataRepresentations](https://arxiv.org/pdf/1808.01462.pdf)\n",
    "- [VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition](https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf)\n",
    "- [FusionNet: 3D Object Classification Using MultipleData Representations](http://3ddl.cs.princeton.edu/2016/papers/Hegde_Zadeh.pdf)\n",
    "- [Uniformizing Techniques to Process CT scans with 3D CNNs for Tuberculosis Prediction](https://arxiv.org/abs/2007.13224)\n",
    "\n",
    "Basic code\n",
    "\n",
    "- [Zunair, H.(2020). Tutorial on 3D Image Classification](https://github.com/hasibzunair/3D-image-classification-tutorial/tree/master)\n",
    "\n",
    "Additional references\n",
    "\n",
    "- [UNet3D: Create 3-D U-Net convolutional neural network for semantic segmentation of volumetric images](https://se.mathworks.com/help/vision/ref/unet3d.html)\n",
    "- [PlatiPy: Processing Library and Analysis Toolkit for Medical Imaging in Python](https://pyplati.github.io/platipy/getting_started.html)\n",
    "- [A series of Russian articles about Self-Supervised Learning on Habr](https://habr.com/ru/articles/704710/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d3371-a5c1-4373-be23-18713adacf58",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836a949-c866-474e-af4d-2f5ed4a8a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.framework.tensor_shape import TensorShape\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440aad5-a4e7-4211-aff9-344d6819b4be",
   "metadata": {},
   "source": [
    "In case you are running code in Google Colab, run the cell below to use Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ceb281-ce63-4dd8-94ed-0590e02b8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2d8ae-a170-4670-8de1-9ecceb99d7c1",
   "metadata": {},
   "source": [
    "For the code to work correctly if it is run in the Colab environment, this project uses the path-variable. Replace the path in the variable with the commented one in case you are running the code in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381877d-af66-4e9d-b5f1-c5e57aa53a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_path=os.path.join(os.getcwd()) #os.path.join(os.getcwd(), 'drive', 'MyDrive', 'Your_Folder')\n",
    "print(curr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7819ef-610e-4312-803c-dcab4497d986",
   "metadata": {},
   "source": [
    "## Downloading the MosMedData: Chest CT Scans with COVID-19 Related Findings\n",
    "\n",
    "In this project, a subset of the\n",
    "[MosMedData: Chest CT Scans with COVID-19 Related Findings](https://www.medrxiv.org/content/10.1101/2020.05.20.20100362v1) is used.\n",
    "This dataset consists of lung CT scans with COVID-19 related findings, as well as without such findings.\n",
    "\n",
    "The associated radiological findings of the CT scans will be used as labels to build\n",
    "a classifier to predict presence of viral pneumonia.\n",
    "Hence, the final task is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e80900-c8b3-42b4-9e83-e3360014f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download url of normal CT scans.\n",
    "url = \"https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-0.zip\"\n",
    "filename = os.path.join(curr_path, \"CT-0.zip\")\n",
    "keras.utils.get_file(origin=url, cache_dir=curr_path)\n",
    "\n",
    "# Download url of abnormal CT scans.\n",
    "url = \"https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-23.zip\"\n",
    "filename = os.path.join(curr_path, \"CT-23.zip\")\n",
    "keras.utils.get_file(origin=url, cache_dir=curr_path)\n",
    "\n",
    "# Make a directory to store the data.\n",
    "os.makedirs(os.path.join(curr_path, \"MosMedData\"))\n",
    "\n",
    "# Unzip data in the newly created directory.\n",
    "with zipfile.ZipFile(os.path.join(curr_path, \"datasets\", \"CT-0.zip\"), \"r\") as z_fp:\n",
    "    z_fp.extractall(os.path.join(curr_path, \"MosMedData\"))\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(curr_path, \"datasets\", \"CT-23.zip\"), \"r\") as z_fp:\n",
    "    z_fp.extractall(os.path.join(curr_path, \"MosMedData\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ccc89-20e2-4aad-bd6e-6d865c94eaa3",
   "metadata": {},
   "source": [
    "## Loading data and preprocessing\n",
    "\n",
    "The files are provided in Nifti format with the extension .nii. To read the\n",
    "scans, the `nibabel` package is used.\n",
    "You can install the package via `pip install nibabel`. CT scans store raw voxel\n",
    "intensity in Hounsfield units (HU). They range from -1024 to above 2000 in this dataset.\n",
    "Above 400 are bones with different radiointensity, so this is used as a higher bound. A threshold\n",
    "between -1000 and 400 is commonly used to normalize CT scans.\n",
    "\n",
    "To process the data, the following are done:\n",
    "\n",
    "* To fix the orientation, volumes are first rotated by 90 degrees for non-platipy datasets\n",
    "* The HU values are scaled to be between 0 and 1.\n",
    "* Also width, height and depth are resized.\n",
    "\n",
    "Here several helper functions are defined to process the data. These functions\n",
    "will be used when building training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507290b-750c-477e-8f4b-86b13d4995ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = nib.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min = -1000\n",
    "    max = 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "\n",
    "def resize_volume(img, platipy = False):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 64\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    angle = 90\n",
    "    if platipy:\n",
    "        angle*=-1\n",
    "    img = ndimage.rotate(img, angle, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    # Normalize\n",
    "    volume = normalize(volume)\n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9e025-e914-42c3-bac9-23587d18871e",
   "metadata": {},
   "source": [
    "Let's read the paths of the CT scans from the class directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3ca65-7ab4-417b-bff3-88461b422493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder \"CT-0\" consist of CT scans having normal lung tissue,\n",
    "# no CT-signs of viral pneumonia.\n",
    "normal_scan_paths = [\n",
    "    os.path.join(curr_path, \"MosMedData/CT-0\", x)\n",
    "    for x in os.listdir(os.path.join(curr_path, \"MosMedData/CT-0\"))\n",
    "]\n",
    "# Folder \"CT-23\" consist of CT scans having several ground-glass opacifications,\n",
    "# involvement of lung parenchyma.\n",
    "abnormal_scan_paths = [\n",
    "    os.path.join(curr_path, \"MosMedData/CT-23\", x)\n",
    "    for x in os.listdir(os.path.join(curr_path, \"MosMedData/CT-23\"))\n",
    "]\n",
    "\n",
    "print(\"CT scans with normal lung tissue: \" + str(len(normal_scan_paths)))\n",
    "print(\"CT scans with abnormal lung tissue: \" + str(len(abnormal_scan_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef44c6-03b7-4652-9263-97d74bb34737",
   "metadata": {},
   "source": [
    "## Build train and validation datasets\n",
    "Read the scans from the class directories and assign labels. Downsample the scans to have\n",
    "shape of 128x128x64. Rescale the raw HU values to the range 0 to 1.\n",
    "Lastly, split the dataset into train and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4a84b-f0a4-4929-9881-9da648b34bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process the scans.\n",
    "# Each scan is resized across height, width, and depth and rescaled.\n",
    "abnormal_scans=[]\n",
    "for path in tqdm(abnormal_scan_paths):\n",
    "    abnormal_scans.append(process_scan(path))\n",
    "abnormal_scans = np.array(abnormal_scans)\n",
    "normal_scans=[]\n",
    "for path in tqdm(normal_scan_paths):\n",
    "    normal_scans.append(process_scan(path))\n",
    "normal_scans = np.array(normal_scans)\n",
    "\n",
    "# For the CT scans having presence of viral pneumonia\n",
    "# assign 1, for the normal ones assign 0.\n",
    "abnormal_labels = np.ones(len(abnormal_scans))\n",
    "normal_labels = np.zeros(len(normal_scans))\n",
    "\n",
    "# Split data in the ratio 70-30 for training and validation.\n",
    "x_train = np.concatenate((abnormal_scans[:70], normal_scans[:70]), axis=0)\n",
    "y_train = np.concatenate((abnormal_labels[:70], normal_labels[:70]), axis=0)\n",
    "x_val = np.concatenate((abnormal_scans[70:], normal_scans[70:]), axis=0)\n",
    "y_val = np.concatenate((abnormal_labels[70:], normal_labels[70:]), axis=0)\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (x_train.shape[0], x_val.shape[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b82d9c-abe1-4179-9b8d-46b6ad6e325b",
   "metadata": {},
   "source": [
    "Save the created subsets and flip scans to orient them and subsequent scans similar way (by orienting hearts e.g. to the left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d3e0b-efbd-490e-afc1-8e040885b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(curr_path, 'x_train'), np.flip(x_train, axis = 2))\n",
    "np.save(os.path.join(curr_path, 'y_train'), y_train)\n",
    "np.save(os.path.join(curr_path, 'x_val'), np.flip(x_val, axis = 2))\n",
    "np.save(os.path.join(curr_path, 'y_val'), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebd070-1741-46fb-a87c-06a8fe935340",
   "metadata": {},
   "source": [
    "## Additional datasets\n",
    "To train the autoencoder, additional sets of CT scans of lungs are needed.\n",
    "Thus let's do similar things with new bunch of datasets from the PlatiPy library. By the way, among all the scans of lungs, it contains only the cancer-affected ones.\n",
    "\n",
    "P.S.: This part of the <ins>clean code</ins> hasn't been tested, therefore it may not work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84398471-dfe2-43b5-b216-b160dc1b158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "from platipy.imaging.tests.data import get_lung_nifti\n",
    "from platipy.imaging.projects.bronchus.run import run_bronchus_segmentation\n",
    "from platipy.imaging import ImageVisualiser\n",
    "from platipy.imaging.label.utils import get_com\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from platipy.dicom.download.tcia import (\n",
    "    get_collections,\n",
    "    get_modalities_in_collection,\n",
    "    get_patients_in_collection,\n",
    "    fetch_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fe921-9513-4a93-93bc-7fe8a6102d77",
   "metadata": {},
   "source": [
    "**Behold** the list of available datasets in the Platipy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c196d-9ef6-4176-a863-6b4eed3ef261",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = get_collections()\n",
    "collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd421dc0-8453-453d-aee1-fdf5eac1ea97",
   "metadata": {},
   "source": [
    "Let's select a dataset and see how many elements there are in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0e3c3-7557-429c-ab23-2680a4d31c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'LCTSC'\n",
    "patients = get_patients_in_collection(collection)\n",
    "print(len(patients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bdd72-a520-4128-94ad-cdcfea37e477",
   "metadata": {},
   "source": [
    "Downloading a dataset. It's not very voluminous ([60 subjects](https://www.cancerimagingarchive.net/collection/lctsc/)), therefore this can be done in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4131d32-8e09-41b3-b3d3-50daa13abd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA=[]\n",
    "for patient_id in tqdm(patients):\n",
    "    try:\n",
    "        data = fetch_data(\n",
    "            collection,\n",
    "            patient_ids=[patient_id],\n",
    "            modalities=[\"CT\"],\n",
    "            nifti=True\n",
    "        )\n",
    "        DATA.append(data)\n",
    "    except:\n",
    "        pass\n",
    "print(len(DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841a99b-2a54-42ee-b82b-c405b717166f",
   "metadata": {},
   "source": [
    "Collect all the paths of the CT scans and split them into train and test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d0818-e490-4d26-bfb3-69fb06504601",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCTSC_train_paths = [\n",
    "    os.path.join(curr_path, \"tcia\\\\LCTSC\", x, \"NIFTI\", x.upper().replace('-', '_'), \"IMAGES\", os.listdir(os.path.join(\"tcia\\\\LCTSC\", x, \"NIFTI\", x.upper().replace('-', '_'), \"IMAGES\"))[0])\n",
    "    for x in os.listdir(\"tcia\\\\LCTSC\") if \"Train\" in x\n",
    "]\n",
    "LCTSC_test_paths = [\n",
    "    os.path.join(curr_path, \"tcia\\\\LCTSC\", x, \"NIFTI\", x.upper().replace('-', '_'), \"IMAGES\", os.listdir(os.path.join(\"tcia\\\\LCTSC\", x, \"NIFTI\", x.upper().replace('-', '_'), \"IMAGES\"))[0])\n",
    "    for x in os.listdir(\"tcia\\\\LCTSC\") if \"Test\" in x\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc6015-c249-498c-9be1-59a089bcf835",
   "metadata": {},
   "source": [
    "Read scans from the directories described above in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f95ebc-4d95-4e3b-be10-5e2a52833e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCTSC_train_scans=[]\n",
    "for path in tqdm(LCTSC_train_paths):\n",
    "    LCTSC_train_scans.append(process_scan(path, platipy=True))\n",
    "LCTSC_train_scans = np.array(LCTSC_train_scans)\n",
    "LCTSC_test_scans=[]\n",
    "for path in tqdm(LCTSC_test_paths):\n",
    "    LCTSC_test_scans.append(process_scan(path, platipy=True))\n",
    "LCTSC_test_scans = np.array(LCTSC_test_scans)\n",
    "\n",
    "LCTSC_train = LCTSC_train_scans\n",
    "LCTSC_test = LCTSC_test_scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59c313-7e1c-443f-919e-2349718df35e",
   "metadata": {},
   "source": [
    "Save the created subsets and flip scans if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae24f0f-7802-43fe-aa4b-162a41433ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('LCTSC_train', np.flip(LCTSC_train, axis = 3))\n",
    "np.save('LCTSC_test', LCTSC_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33511eb-667b-482f-99db-2c5e1576cba9",
   "metadata": {},
   "source": [
    "Similarly, select another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ca205-a5d6-4868-af4e-5a73f02909c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'Lung-PET-CT-Dx'\n",
    "patients = get_patients_in_collection(collection)\n",
    "print(len(patients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12101a30-4583-456e-8d74-a60ccd470bfe",
   "metadata": {},
   "source": [
    "Let's download a dataset. It's really bulky ([355 subjects](https://www.cancerimagingarchive.net/collection/lung-pet-ct-dx/)), this could be difficult to be done in one go. Therefore, \"the design of the slices\" was added for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383fff2-c02e-462a-b8ab-936421a808f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA=[]\n",
    "for patient_id in tqdm(patients[:]):\n",
    "    try:\n",
    "        data = fetch_data(\n",
    "            collection,\n",
    "            patient_ids=[patient_id],\n",
    "            modalities=[\"CT\"],\n",
    "            nifti=True\n",
    "        )\n",
    "        DATA.append(data)\n",
    "    except:\n",
    "        pass\n",
    "print(len(DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fe9ca-a906-4be0-8596-603a767f675a",
   "metadata": {},
   "source": [
    "Browse and collect paths of the CT scans and divide them into 4 groups according to the cancer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63558c-b0a4-4f5a-af5c-d654ab545b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_paths=[\n",
    "    os.path.join(curr_path, \"tcia\\\\Lung-PET-CT-Dx\", x, \"NIFTI\", x.upper().replace('-', '_'), \"IMAGES\")\n",
    "    for x in os.listdir(\"tcia\\\\Lung-PET-CT-Dx\") if (\"A\" in x) and os.path.exists(os.path.join(curr_path, \"tcia\\\\Lung-PET-CT-Dx\", x, \"NIFTI\"))\n",
    "]\n",
    "PET_A_paths=[]\n",
    "for p in temp_paths:\n",
    "    for x in os.listdir(p):\n",
    "        PET_A_paths.append(os.path.join(p, x))\n",
    "temp_paths=[\n",
    "    os.path.join(curr_path, \"tcia\\\\Lung-PET-CT-Dx\", x, \"NIFTI\", x.upper().replace('-', '_'), \"IMAGES\")\n",
    "    for x in os.listdir(\"tcia\\\\Lung-PET-CT-Dx\") if (\"B\" in x) and os.path.exists(os.path.join(curr_path, \"tcia\\\\Lung-PET-CT-Dx\", x, \"NIFTI\"))\n",
    "]\n",
    "PET_B_paths=[]\n",
    "for p in temp_paths:\n",
    "    for x in os.listdir(p):\n",
    "        PET_B_paths.append(os.path.join(p, x))\n",
    "temp_paths=[\n",
    "    os.path.join(curr_path, \"tcia\\\\Lung-PET-CT-Dx\", x, \"NIFTI\", x.upper().replace('-', '_'), \"IMAGES\")\n",
    "    for x in os.listdir(\"tcia\\\\Lung-PET-CT-Dx\") if (\"E\" in x) and os.path.exists(os.path.join(curr_path, \"tcia\\\\Lung-PET-CT-Dx\", x, \"NIFTI\"))\n",
    "]\n",
    "PET_E_paths=[]\n",
    "for p in temp_paths:\n",
    "    for x in os.listdir(p):\n",
    "        PET_E_paths.append(os.path.join(p, x))\n",
    "temp_paths=[\n",
    "    os.path.join(curr_path, \"tcia\\\\Lung-PET-CT-Dx\", x, \"NIFTI\", x.upper().replace('-', '_'), \"IMAGES\")\n",
    "    for x in os.listdir(\"tcia\\\\Lung-PET-CT-Dx\") if (\"G\" in x) and os.path.exists(os.path.join(curr_path, \"tcia\\\\Lung-PET-CT-Dx\", x, \"NIFTI\"))\n",
    "]\n",
    "PET_G_paths=[]\n",
    "for p in temp_paths:\n",
    "    for x in os.listdir(p):\n",
    "        PET_G_paths.append(os.path.join(p, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708539c-3c67-4cfe-8cbe-fdcba4fc9941",
   "metadata": {},
   "source": [
    "Read scans from the directories described above similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84f403-d617-40b5-82fd-cf6bd0491bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PET_A_scans=[]\n",
    "for path in tqdm(PET_A_paths):\n",
    "    PET_A_scans.append(process_scan(path))\n",
    "PET_A_scans = np.array(PET_A_scans)\n",
    "PET_B_scans=[]\n",
    "for path in tqdm(PET_B_paths):\n",
    "    PET_B_scans.append(process_scan(path))\n",
    "PET_B_scans = np.array(PET_B_scans)\n",
    "PET_E_scans=[]\n",
    "for path in tqdm(PET_E_paths):\n",
    "    PET_E_scans.append(process_scan(path))\n",
    "PET_E_scans = np.array(PET_E_scans)\n",
    "PET_G_scans=[]\n",
    "for path in tqdm(PET_G_paths):\n",
    "    PET_G_scans.append(process_scan(path))\n",
    "PET_G_scans = np.array(PET_G_scans)\n",
    "\n",
    "PET_A = PET_A_scans\n",
    "PET_B = PET_B_scans\n",
    "PET_E = PET_E_scans\n",
    "PET_G = PET_G_scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc278470-4080-41fd-8fe5-dd266903c4fc",
   "metadata": {},
   "source": [
    "Save the created subsets and flip scans if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585a4b3-5f21-4c86-ad2c-640241171944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('PET_A', np.flip(PET_A))\n",
    "np.save('PET_B', np.flip(PET_B, axis = 3))\n",
    "np.save('PET_E', np.flip(PET_E, axis = 3))\n",
    "np.save('PET_G', np.flip(PET_G, axis = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70163caa-c053-40e3-9c4d-c3ab2b3ef9d4",
   "metadata": {},
   "source": [
    "To reduce the file size of A-group of scans, split this dataset into 4 \"subdatasets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf6a9e-edac-4cf0-8f15-963f19f94189",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = PET_A.shape[0]//4\n",
    "PET_A1=PET_A[:delta]\n",
    "PET_A2=PET_A[delta:2*delta]\n",
    "PET_A3=PET_A[2*delta:3*delta]\n",
    "PET_A4=PET_A[3*delta:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f8ad7-4677-4149-b783-0797a5a3d7b8",
   "metadata": {},
   "source": [
    "Save the created subsets and flip scans if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d08fb-915e-4f7f-9d2b-9c91efc23d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('PET_A1', np.flip(PET_A1, axis = 3))\n",
    "np.save('PET_A2', np.flip(PET_A2, axis = 3))\n",
    "np.save('PET_A3', np.flip(PET_A3, axis = 3))\n",
    "np.save('PET_A4', np.flip(PET_A4, axis = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce43698-c1c8-4037-8c98-88ed7b65c0fe",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "The CT scans also augmented by rotating at random angles during training. Since\n",
    "the data is stored in rank-3 tensors of shape `(samples, height, width, depth)`,\n",
    "we add a dimension of size 1 at axis 4 to be able to perform 3D convolutions on\n",
    "the data. The new shape is thus `(samples, height, width, depth, 1)`. There are\n",
    "different kinds of preprocessing and augmentation techniques out there,\n",
    "this example shows a few simple ones to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14ea47-6227-4d2d-b7a2-fc96591750af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "@tf.function\n",
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "\n",
    "    VOLUME_SHAPE = volume.shape\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    augmented_volume.set_shape(VOLUME_SHAPE)\n",
    "    return augmented_volume\n",
    "\n",
    "def train_preprocessing(volume, label):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "def rotatePi(volume):\n",
    "    angle = 180\n",
    "    # rotate volume\n",
    "    volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "    volume[volume < 0] = 0\n",
    "    volume[volume > 1] = 1\n",
    "    return volume\n",
    "def LCTSC_preprocessing(volume, label):\n",
    "    \"\"\"Process LCTSC data by rotating and adding a channel.\"\"\"\n",
    "    volume = tf.numpy_function(rotatePi, [volume], tf.float32)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "def PET_preprocessing(volume, label):\n",
    "    \"\"\"Process PET data by rotating and adding a channel.\"\"\"\n",
    "    volume = tf.numpy_function(rotatePi, [volume], tf.float32)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b79987-8dc6-455e-81c3-301ec9369db9",
   "metadata": {},
   "source": [
    "Below are other augmentation methods provided in the class structure which are used in this project.\n",
    "> \"Cut\" cuts out some part of the image, darkening the area on it, and \"noise\" - makes noise on it, pretty simple.<br>\n",
    "Â© *Andranik Fakirian, in his [blog](https://www.tumblr.com/andranikfakirian/768855980222087168/project-mlpneumonia-augmentations?source=share) (2024)*\n",
    "\n",
    "Also there is presented a simplified preprocessing method, which is general to all datasets in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46242df4-e320-429a-b57a-8850ffdb4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(volume, pieces, lenmin, lenmax, widthmin, widthmax):\n",
    "    for v in volume:\n",
    "        for i in range(pieces):\n",
    "            l = np.random.randint(lenmin, lenmax)\n",
    "            w = np.random.randint(widthmin, widthmax)\n",
    "            shape = v.shape\n",
    "            x = np.random.randint(0, shape[0]-1-l)\n",
    "            y = np.random.randint(0, shape[1]-1-w)\n",
    "            v[x:x+l, y:y+w]=0\n",
    "    return volume\n",
    "def noise(volume, mu, sig):\n",
    "    for v in volume:\n",
    "        shape=v.shape\n",
    "        noise=np.repeat(np.expand_dims(np.random.normal(mu, sig, shape[:-1]), axis=2), shape[-1], 2)\n",
    "        v+=noise\n",
    "        v[v < 0] = 0\n",
    "        v[v > 1] = 1\n",
    "    return volume\n",
    "def ordinary_preprocessing(volume, label):\n",
    "    \"\"\"Process data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "class Augmentation():\n",
    "    mu: float\n",
    "    sig: float\n",
    "    pieces: int\n",
    "    lenmin: int\n",
    "    lenmax: int\n",
    "    widthmin: int\n",
    "    widthmax: int\n",
    "    def __init__(self, mu = 0, sig = 5e-2, pieces = 1, lenmin = 0, lenmax = 32, widthmin = 0, widthmax = 32):\n",
    "        self.mu = mu\n",
    "        self.sig = sig\n",
    "        self.pieces = pieces\n",
    "        self.lenmin = lenmin\n",
    "        self.lenmax = lenmax\n",
    "        self.widthmin = widthmin\n",
    "        self.widthmax = widthmax\n",
    "    def Noise(self, volume):\n",
    "        augmented_volume = noise(volume, self.mu, self.sig)\n",
    "        return augmented_volume\n",
    "    def Cut(self, volume):\n",
    "        augmented_volume = cut(volume, self.pieces, self.lenmin, self.lenmax, self.widthmin, self.widthmax)\n",
    "        return augmented_volume\n",
    "    def Mix(self, volume):\n",
    "        augmented_volume = cut(noise(volume, self.mu, self.sig), self.pieces, self.lenmin, self.lenmax, self.widthmin, self.widthmax)\n",
    "        return augmented_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5421af8-db62-4a78-9e50-3129c5270e0f",
   "metadata": {},
   "source": [
    "Loading datasets to build training and validation datasets for the autoencoder.\n",
    "\n",
    "P.S.: This part of the <ins>clean code</ins> also hasn't been tested, therefore it may not work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59db1e-a7e6-4945-8740-f6a35fc1f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(os.path.join(curr_path, 'x_train.npy'), encoding = 'bytes')\n",
    "x_val = np.load(os.path.join(curr_path, 'x_val.npy'), encoding = 'bytes')\n",
    "LCTSC_train = np.load(os.path.join(curr_path, 'LCTSC_train.npy'), encoding = 'bytes')\n",
    "LCTSC_test = np.load(os.path.join(curr_path, 'LCTSC_test.npy'), encoding = 'bytes')\n",
    "PET_G = np.load(os.path.join(curr_path, 'PET_G.npy'), encoding = 'bytes')\n",
    "PET_E = np.load(os.path.join(curr_path, 'PET_E.npy'), encoding = 'bytes')\n",
    "PET_B = np.load(os.path.join(curr_path, 'PET_B.npy'), encoding = 'bytes')\n",
    "#PET_A1 = np.flip(np.load(os.path.join(curr_path, 'PET_A1.npy'), encoding = 'bytes')\n",
    "#PET_A2 = np.flip(np.load(os.path.join(curr_path, 'PET_A2.npy'), encoding = 'bytes')\n",
    "#PET_A3 = np.flip(np.load(os.path.join(curr_path, 'PET_A3.npy'), encoding = 'bytes')\n",
    "PET_A4 = np.load(os.path.join(curr_path, 'PET_A4.npy'), encoding = 'bytes')\n",
    "#PET_A = np.concatenate((PET_A1, PET_A2, PET_A3, PET_A4), axis = 0)\n",
    "#del PET_A1, PET_A2, PET_A3, PET_A4\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575fca1-5ae6-4ae9-8662-3a773c9ac015",
   "metadata": {},
   "source": [
    "Splitting a mixed dataset into training and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72a31d-ca8f-46f4-913e-a624d056778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "encoder_general = np.concatenate((LCTSC_train, LCTSC_test, PET_G, PET_E, PET_B[:40], PET_A4, x_train, x_val), axis = 0)\n",
    "del LCTSC_train, LCTSC_test, PET_G, PET_E, PET_B, PET_A4, x_train, x_val\n",
    "gc.collect()\n",
    "encoder_train, encoder_val = train_test_split(encoder_general, test_size=0.2, random_state=42)\n",
    "del encoder_general\n",
    "gc.collect()\n",
    "len_encoder_train, len_encoder_val = len(encoder_train), len(encoder_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15743d0-c163-4c55-9786-b12bc2e8c154",
   "metadata": {},
   "source": [
    "Divide the training dataset for <ins>RAM-saving focus</ins> and save the created subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a6a9d-4127-43f3-a4cf-fcc0aa601679",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(curr_path, 'encoder_train1'), encoder_train[:len_encoder_train//4])\n",
    "np.save(os.path.join(curr_path, 'encoder_train2'), encoder_train[len_encoder_train//4:2*len_encoder_train//4])\n",
    "np.save(os.path.join(curr_path, 'encoder_train3'), encoder_train[2*len_encoder_train//4:3*len_encoder_train//4])\n",
    "np.save(os.path.join(curr_path, 'encoder_train4'), encoder_train[3*len_encoder_train//4:])\n",
    "np.save(os.path.join(curr_path, 'encoder_val'), encoder_val)\n",
    "del encoder_train, encoder_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c50c84-49c5-401b-b55b-fa36e5314b47",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "Load prepared training and validation subsets and create complex datasets consisting of augmented scans and non-augmented ones as labels.\n",
    "\n",
    "P.S.: The previously mentioned <ins>focus</ins> is to gradually load the training subset into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ea97f-c147-4f80-844e-e7048b7c71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Augmentation()\n",
    "encoder_train = np.load(os.path.join(curr_path, 'encoder_train1.npy'), encoding = 'bytes')\n",
    "encoder_train_length = len(encoder_train)\n",
    "encoder_train_length_s = encoder_train_length\n",
    "encoder_train_loader_s = tf.data.Dataset.from_tensor_slices((encoder_train.copy(), np.concatenate((A.Cut(encoder_train[:encoder_train_length//3]), A.Noise(encoder_train[encoder_train_length//3:2*encoder_train_length//3]), A.Mix(encoder_train[2*encoder_train_length//3:])), axis = 0))[::-1])\n",
    "del encoder_train\n",
    "gc.collect()\n",
    "encoder_train = np.load(os.path.join(curr_path, 'encoder_train2.npy'), encoding = 'bytes')\n",
    "encoder_train_length = len(encoder_train)\n",
    "encoder_train_length_s += encoder_train_length\n",
    "encoder_train_loader = tf.data.Dataset.from_tensor_slices((encoder_train.copy(), np.concatenate((A.Cut(encoder_train[:encoder_train_length//3]), A.Noise(encoder_train[encoder_train_length//3:2*encoder_train_length//3]), A.Mix(encoder_train[2*encoder_train_length//3:])), axis = 0))[::-1])\n",
    "del encoder_train\n",
    "gc.collect()\n",
    "encoder_train_loader_s = encoder_train_loader_s.concatenate(encoder_train_loader)\n",
    "del encoder_train_loader\n",
    "gc.collect()\n",
    "encoder_train = np.load(os.path.join(curr_path, 'encoder_train3.npy'), encoding = 'bytes')\n",
    "encoder_train_length = len(encoder_train)\n",
    "encoder_train_length_s += encoder_train_length\n",
    "encoder_train_loader = tf.data.Dataset.from_tensor_slices((encoder_train.copy(), np.concatenate((A.Cut(encoder_train[:encoder_train_length//3]), A.Noise(encoder_train[encoder_train_length//3:2*encoder_train_length//3]), A.Mix(encoder_train[2*encoder_train_length//3:])), axis = 0))[::-1])\n",
    "del encoder_train\n",
    "gc.collect()\n",
    "encoder_train_loader_s = encoder_train_loader_s.concatenate(encoder_train_loader)\n",
    "del encoder_train_loader\n",
    "gc.collect()\n",
    "encoder_train = np.load(os.path.join(curr_path, 'encoder_train4.npy'), encoding = 'bytes')\n",
    "encoder_train_length = len(encoder_train)\n",
    "encoder_train_length_s += encoder_train_length\n",
    "encoder_train_loader = tf.data.Dataset.from_tensor_slices((encoder_train.copy(), np.concatenate((A.Cut(encoder_train[:encoder_train_length//3]), A.Noise(encoder_train[encoder_train_length//3:2*encoder_train_length//3]), A.Mix(encoder_train[2*encoder_train_length//3:])), axis = 0))[::-1])\n",
    "del encoder_train\n",
    "gc.collect()\n",
    "encoder_train_loader_s = encoder_train_loader_s.concatenate(encoder_train_loader)\n",
    "del encoder_train_loader\n",
    "gc.collect()\n",
    "batch_size = 2\n",
    "encoder_train_dataset = (\n",
    "    encoder_train_loader_s.shuffle(encoder_train_length_s)\n",
    "    .map(ordinary_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf824ef-1d64-47ad-bbcb-3f595140170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_val = np.load(os.path.join(curr_path, 'encoder_val.npy'), encoding = 'bytes')\n",
    "encoder_val_length=len(encoder_val)\n",
    "encoder_validation_loader = tf.data.Dataset.from_tensor_slices((encoder_val.copy(), np.concatenate((A.Cut(encoder_val[:encoder_val_length//3]), A.Noise(encoder_val[encoder_val_length//3:2*encoder_val_length//3]), A.Mix(encoder_val[2*encoder_val_length//3:])), axis = 0))[::-1])\n",
    "del encoder_val\n",
    "gc.collect()\n",
    "encoder_validation_dataset = (\n",
    "    encoder_validation_loader.shuffle(encoder_val_length)\n",
    "    .map(ordinary_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf291d-ccc6-4007-bdbc-618d5f6f85ef",
   "metadata": {},
   "source": [
    "Visualize an augmented CT scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac9d5e-5bcb-4c17-b97c-0b767a69a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = encoder_train_dataset.take(1)\n",
    "images, labels = list(data)[0]\n",
    "images = images.numpy()\n",
    "image = images[0]\n",
    "label = labels[0]\n",
    "print(\"Dimension of the CT scan is:\", image.shape)\n",
    "plt.imshow(np.squeeze(image[:, :, 15]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0d853-fe2b-49de-ac06-a0dbd77590d8",
   "metadata": {},
   "source": [
    "More detailed visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e0731-eea8-4e47-9cd5-be5b39aecc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "font={'font.size': 20}\n",
    "plt.rcParams.update(font)\n",
    "fig, ax = plt.subplots(2, figsize=(10, 20))\n",
    "ax[0].imshow(np.squeeze(image[:, :, 15]), cmap=\"gray\")\n",
    "ax[0].set_title('Augmented CT scan')\n",
    "ax[1].imshow(np.squeeze(label[:, :, 15]), cmap=\"gray\")\n",
    "ax[1].set_title('Non-augmented CT scan')\n",
    "plt.show()\n",
    "#fig.savefig('AugCT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6b1ab-beed-4450-8990-0540a9b20c42",
   "metadata": {},
   "source": [
    "Since a CT scan has many slices, let's visualize a montage of the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97264d80-7882-45bc-933c-fce796b1c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slices(num_rows, num_columns, width, height, data):\n",
    "    \"\"\"Plot a montage of 20 CT slices\"\"\"\n",
    "    data = np.rot90(np.array(data))\n",
    "    data = np.transpose(data)\n",
    "    data = np.reshape(data, (num_rows, num_columns, width, height))\n",
    "    rows_data, columns_data = data.shape[0], data.shape[1]\n",
    "    heights = [slc[0].shape[0] for slc in data]\n",
    "    widths = [slc.shape[1] for slc in data[0]]\n",
    "    fig_width = 12.0\n",
    "    fig_height = fig_width * sum(heights) / sum(widths)\n",
    "    f, axarr = plt.subplots(\n",
    "        rows_data,\n",
    "        columns_data,\n",
    "        figsize=(fig_width, fig_height),\n",
    "        gridspec_kw={\"height_ratios\": heights},\n",
    "    )\n",
    "    for i in range(rows_data):\n",
    "        for j in range(columns_data):\n",
    "            axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n",
    "            axarr[i, j].axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize montage of slices.\n",
    "# 4 rows and 10 columns for 100 slices of the CT scan.\n",
    "plot_slices(4, 10, 128, 128, image[:, :, :40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e666c4-93f1-410b-b5c0-1794a12a34ec",
   "metadata": {},
   "source": [
    "## Define a 3D convolutional neural network\n",
    "\n",
    "To make the model easier to understand, it's structured into blocks.\n",
    "The architecture of the 3D CNN classifier used in this project\n",
    "is based on [this paper](https://arxiv.org/abs/2007.13224) and the architecture of the autoencoder is based on the [UNet3D](https://se.mathworks.com/help/vision/ref/unet3d.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9093e-595a-4761-b47c-f38f70573a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coder():\n",
    "    def __init__(self, width=128, height=128, depth=64, latent_dim = 128):\n",
    "        self.width=width\n",
    "        self.height=height\n",
    "        self.depth=depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = None\n",
    "        self.classifier = None\n",
    "    def get_encoder(self):\n",
    "        \"\"\"Build a 3D convolutional neural network encoder.\"\"\"\n",
    "\n",
    "        #Encode\n",
    "        inputs = keras.Input((self.width, self.height, self.depth, 1), name=\"encoder_input\")\n",
    "\n",
    "        x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\", padding='same')(inputs)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\", padding='same')(x)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\", padding='same')(x)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Flatten()(x)\n",
    "        latent = layers.Dense(self.latent_dim, activation=\"relu\", name=\"latent_space\")(x)\n",
    "\n",
    "        encoder = keras.Model(inputs, latent, name=\"3d_encoder\")\n",
    "        return encoder\n",
    "    def get_decoder(self):\n",
    "        \"\"\"Build a 3D convolutional neural network decoder.\"\"\"\n",
    "        #Decode\n",
    "        latent_inputs = tf.keras.Input(shape=(self.latent_dim,), name=\"decoder_input\")\n",
    "        x = layers.Dense(self.width // 8 * self.height // 8 * self.depth // 8 * 256, activation=\"relu\")(latent_inputs)\n",
    "        x = layers.Reshape((self.width // 8, self.height // 8, self.depth // 8, 256))(x)\n",
    "\n",
    "        x = layers.Conv3DTranspose(filters=256, kernel_size=3, activation=\"relu\", padding='same')(x)\n",
    "        x = layers.UpSampling3D(size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Conv3DTranspose(filters=128, kernel_size=3, activation=\"relu\", padding='same')(x)\n",
    "        x = layers.UpSampling3D(size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Conv3DTranspose(filters=64, kernel_size=3, activation=\"relu\", padding='same')(x)\n",
    "        x = layers.UpSampling3D(size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        outputs = layers.Conv3DTranspose(filters=1, kernel_size=3, activation=\"sigmoid\", padding='same')(x)\n",
    "\n",
    "        decoder = keras.Model(latent_inputs, outputs, name=\"3d_decoder\")\n",
    "        return decoder\n",
    "    def get_autoencoder(self, lr_schedule = None):\n",
    "        \"\"\"Build a full model of autoencoder\"\"\"\n",
    "        self.encoder = self.get_encoder()\n",
    "        decoder = self.get_decoder()\n",
    "        autoencoder = keras.Model(self.encoder.input, decoder(self.encoder.output), name=\"3d_autoencoder\")\n",
    "        autoencoder.summary()\n",
    "        if not (lr_schedule==None):\n",
    "            autoencoder.compile(\n",
    "                loss=\"mse\",\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            )\n",
    "        return autoencoder\n",
    "    def get_classifier(self, lr_schedule, trainable=False):\n",
    "        \"\"\"Build a classifier based on the encoder\"\"\"\n",
    "        if self.encoder is None:\n",
    "            self.encoder = self.get_encoder()\n",
    "\n",
    "        for layer in self.encoder.layers: #Freeze encoder weights\n",
    "            layer.trainable = trainable\n",
    "\n",
    "        inputs = self.encoder.input\n",
    "        x = self.encoder.output\n",
    "\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        outputs = layers.Dense(1, activation=\"sigmoid\", name=\"classification_output\")(x)\n",
    "\n",
    "        self.classifier = keras.Model(inputs, outputs, name=\"3d_classifier\")\n",
    "        self.classifier.summary()\n",
    "        self.classifier.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            metrics=[\"acc\"],\n",
    "        )\n",
    "        return self.classifier\n",
    "\n",
    "    def load_encoder_weights(self, weights_path):\n",
    "        \"\"\"Load trained encoder weights\"\"\"\n",
    "        if self.encoder is None:\n",
    "            self.encoder = self.get_encoder()\n",
    "        self.encoder.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97a11b-9cb7-4f9a-aaa5-6ba36603a8af",
   "metadata": {},
   "source": [
    "## Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51294fe1-213d-460e-9445-7605c346e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model.\n",
    "cod = Coder(width=128, height=128, depth=64, latent_dim=128)\n",
    "\n",
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "autoencoder = cod.get_autoencoder(lr_schedule)\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb_ae = keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(curr_path, '3d_image_autoencoder.weights.h5'), save_best_only=True, save_weights_only=True\n",
    ")\n",
    "early_stopping_cb_ae = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "csv_logger_cb_ae = keras.callbacks.CSVLogger(os.path.join(curr_path, '3d_image_autoencoder.csv'))\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 100\n",
    "autoencoder.fit(\n",
    "    encoder_train_dataset,\n",
    "    validation_data=encoder_validation_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb_ae, early_stopping_cb_ae, csv_logger_cb_ae],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc5b6b-1243-49cb-9a77-a25fdd363e0c",
   "metadata": {},
   "source": [
    "## Visualizing autoencoder performance\n",
    "\n",
    "Here the model loss for the training and the validation sets is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f97f9-f3cd-4233-9194-68afa1349f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "history = pd.read_csv(os.path.join(curr_path, '3d_image_autoencoder.csv'))\n",
    "history['epoch']+=1\n",
    "val_mid_idx = history[['val_loss']].idxmin()\n",
    "ax.plot(history['epoch'], history['loss'], zorder=2, label = 'train')\n",
    "ax.plot(history['epoch'], history['val_loss'], zorder=2, label = 'val')\n",
    "ax.scatter(history.iloc[val_mid_idx]['epoch'], history.iloc[val_mid_idx]['val_loss'], zorder = 3, c = 'crimson', marker = 's', s=20, label = 'Min val loss = {:.3f}'.format(history.iloc[val_mid_idx]['val_loss'].iloc[0]))\n",
    "ax.plot([history.iloc[val_mid_idx]['epoch'].iloc[0], history.iloc[val_mid_idx]['epoch'].iloc[0]], [0, history.iloc[val_mid_idx]['val_loss'].iloc[0]], zorder = 2, c = 'crimson', ls = '--', lw = 1)\n",
    "ax.set_title(\"Model loss\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc=0)\n",
    "ax.grid(zorder = 1, alpha = 0.5)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(curr_path, 'AE_log.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49a766-15f6-4484-9ad0-90aa5bb1dc62",
   "metadata": {},
   "source": [
    "To present the performance of the autoencoder, let's predict the image using augmented and compare it with the initial one.\n",
    "\n",
    "Firstly, load the weights of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c0668-7eaa-441e-a9cf-fab3bb53d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "cod = Coder(width=128, height=128, depth=64, latent_dim=128)\n",
    "autoencoder = cod.get_autoencoder(lr_schedule)\n",
    "autoencoder.load_weights(os.path.join(curr_path, '3d_image_autoencoder.weights.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05f47c-624c-4c70-8c13-b568d63411ba",
   "metadata": {},
   "source": [
    "And visualize it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0398f-b075-465f-b32e-4c058cbc7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encoder_validation_dataset.take(1)\n",
    "images, labels = list(data)[0]\n",
    "images = images.numpy()\n",
    "image = images[0]\n",
    "label = labels[0]\n",
    "\n",
    "prediction = autoencoder.predict(np.expand_dims(image, axis = 0))\n",
    "\n",
    "font={'font.size': 20}\n",
    "plt.rcParams.update(font)\n",
    "fig, ax = plt.subplots(3, figsize=(5, 15))\n",
    "ax[0].imshow(np.squeeze(image[:, :, 15]), cmap=\"gray\")\n",
    "ax[0].set_title('Augmented CT scan')\n",
    "ax[1].imshow(np.squeeze(prediction[:, :, :, 15]), cmap=\"gray\")\n",
    "ax[1].set_title('Predicted CT scan')\n",
    "ax[2].imshow(np.squeeze(label[:, :, 15]), cmap=\"gray\")\n",
    "ax[2].set_title('Non-augmented CT scan')\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(curr_path, 'AE_res.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4854c6-8c55-43ac-9dab-56706fc19e18",
   "metadata": {},
   "source": [
    "## Creating datasets for the classifier\n",
    "We are not doing anything new. Firstly, loading previously prepared training and validation subsets of the MosMedData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16129ef2-7b3b-4876-b70d-543c6c8958ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(os.path.join(curr_path, 'x_train.npy'), encoding = 'bytes')\n",
    "x_val = np.load(os.path.join(curr_path, 'x_val.npy'), encoding = 'bytes')\n",
    "y_train = np.load(os.path.join(curr_path, 'y_train.npy'), encoding = 'bytes')\n",
    "y_val = np.load(os.path.join(curr_path, 'y_val.npy'), encoding = 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff0033-7601-4827-bc41-ee027a19c3d6",
   "metadata": {},
   "source": [
    "Create complex datasets consisting of non-augmented scans and \"indicators\" of the pneumonia presence afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466522a-6f0f-4970-abec-5888dddf44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train))\n",
    "    .map(ordinary_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_val))\n",
    "    .map(ordinary_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "\n",
    "del x_train, y_train, x_val, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262124f3-5ef5-4b31-95cd-c7b719ec0c60",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fa5f7-d2cb-4f4b-b393-1048c3d69f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained encoder\n",
    "cod = Coder(width=128, height=128, depth=64, latent_dim=128)\n",
    "cod.load_encoder_weights(os.path.join(curr_path, '3d_image_autoencoder.weights.h5'))\n",
    "\n",
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# Build the classifier with unfreezed weights for additional training\n",
    "classifier = cod.get_classifier(lr_schedule, trainable=True)\n",
    "# Build the classifier with freezed weights\n",
    "#classifier = cod.get_classifier(lr_schedule, trainable=False)\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb_cl = keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(curr_path, '3d_image_classifier_Unfreezed.weights.h5'), save_best_only=True, save_weights_only=True, monitor = 'val_acc'\n",
    ")\n",
    "early_stopping_cb_cl = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=30)\n",
    "csv_logger_cb_cl = keras.callbacks.CSVLogger(os.path.join(curr_path, '3d_image_classifier_Unfreezed.csv'))\n",
    "\n",
    "epochs = 100\n",
    "classifier.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb_cl, early_stopping_cb_cl, csv_logger_cb_cl],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0a624-7c6a-414e-a4d4-67471ed0ff90",
   "metadata": {},
   "source": [
    "The author of the basic code also trained his classification CNN using a larger [pneumonia dataset](https://mosmed.ai/datasets/covid191110/) of over 1000 CT scans. And he achieved the result with an accuracy of 83%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad7b58-5a8e-490b-96de-aac13612a20d",
   "metadata": {},
   "source": [
    "## Visualizing classifier performance\n",
    "\n",
    "Here the model accuracy and loss for the training and the validation sets are plotted.\n",
    "Since the validation set is class-balanced, accuracy provides an unbiased representation\n",
    "of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0860d-259f-4f50-a80f-5bbfecbb6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2, 1, figsize=(7, 8))\n",
    "history = pd.read_csv(os.path.join(curr_path, '3d_image_classifier_Unfreezed.csv'))\n",
    "history['epoch']+=1\n",
    "val_mid_idx = history[['val_loss']].idxmin()\n",
    "valacc_max_idx = history[['val_acc']].idxmax()\n",
    "ax[0].plot(history['epoch'], history['loss'], zorder=2, label = 'train')\n",
    "ax[0].plot(history['epoch'], history['val_loss'], zorder=2, label = 'val')\n",
    "ax[1].plot(history['epoch'], history['acc'], zorder=2, label = 'train')\n",
    "ax[1].plot(history['epoch'], history['val_acc'], zorder=2, label = 'val')\n",
    "ax[0].scatter(history.iloc[val_mid_idx]['epoch'], history.iloc[val_mid_idx]['val_loss'], zorder = 3, c = 'crimson', marker = 's', s=20, label = 'Min val loss = {:.3f}'.format(history.iloc[val_mid_idx]['val_loss'].iloc[0]))\n",
    "ax[1].scatter(history.iloc[valacc_max_idx]['epoch'], history.iloc[valacc_max_idx]['val_acc'], zorder = 3, c = 'crimson', marker = 's', s=20, label = 'Max val acc = {:.3f}'.format(history.iloc[valacc_max_idx]['val_acc'].iloc[0]))\n",
    "ax[0].plot([history.iloc[val_mid_idx]['epoch'].iloc[0], history.iloc[val_mid_idx]['epoch'].iloc[0]], [0, history.iloc[val_mid_idx]['val_loss'].iloc[0]], zorder = 2, c = 'crimson', ls = '--', lw = 1)\n",
    "ax[1].plot([history.iloc[valacc_max_idx]['epoch'].iloc[0], history.iloc[valacc_max_idx]['epoch'].iloc[0]], [0, history.iloc[valacc_max_idx]['val_acc'].iloc[0]], zorder = 2, c = 'crimson', ls = '--', lw = 1)\n",
    "ax[0].set_title(\"Model loss\")\n",
    "ax[1].set_title(\"Model acc\")\n",
    "ax[0].set_xlabel(\"epochs\")\n",
    "ax[1].set_xlabel(\"epochs\")\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[1].set_ylabel('acc')\n",
    "ax[0].legend(loc=0)\n",
    "ax[1].legend(loc=0)\n",
    "ax[0].grid(zorder = 1, alpha = 0.5)\n",
    "ax[1].grid(zorder = 1, alpha = 0.5)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(curr_path, 'CL_log.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298f297-dd6b-4af8-ad71-20d730b319af",
   "metadata": {},
   "source": [
    "## Make predictions on a single CT scan\n",
    "Loading the weights of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad5ecf-3cdd-4f6c-a8ca-654774baf17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod = Coder(width=128, height=128, depth=64, latent_dim=128)\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "classifier = cod.get_classifier(lr_schedule, trainable=False)\n",
    "classifier.load_weights(os.path.join(curr_path, \"3d_image_classifier_Unfreezed.weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169e09b-aaca-4b2a-b8d9-4909eac7d1de",
   "metadata": {},
   "source": [
    "Visualization of a single scan image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab993216-8238-4576-9c4d-511199e88fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = validation_dataset.take(1)\n",
    "images, labels = list(data)[0]\n",
    "images = images.numpy()\n",
    "image = images[0]\n",
    "label = labels.numpy()[0]\n",
    "print(\"Dimension of the CT scan is:\", image.shape)\n",
    "plt.imshow(np.squeeze(image[:, :, 15]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490909df-70e9-42af-8765-6345f7722f44",
   "metadata": {},
   "source": [
    "Model predicition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551f4d9-9f4d-4192-a771-4e4bdae5a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier.predict(np.expand_dims(image, axis = 0))\n",
    "scores = [1 - prediction[0], prediction[0]]\n",
    "print(label)\n",
    "\n",
    "class_names = [\"normal\", \"abnormal\"]\n",
    "for score, name in zip(scores, class_names):\n",
    "    print(\n",
    "        \"This model is %.2f percent confident that CT scan is %s\"\n",
    "        % ((100 * score), name)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
